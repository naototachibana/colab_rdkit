{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chainer-chemistry_colab_transplantation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naototachibana/colab_transplantation/blob/master/chainer_chemistry_colab_transplantation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIihJ7n4oUGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "0d197822-fef4-453c-9bfd-bc75f3a8624d"
      },
      "source": [
        "# 2\n",
        "# rdkitをインストール\n",
        "!curl -Lo rdkit_installer.py https://git.io/fxiPZ\n",
        "import rdkit_installer\n",
        "%time rdkit_installer.install()\n",
        "\n",
        "!pip install chainer-chemistry"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\r100  2415  100  2415    0     0   4480      0 --:--:-- --:--:-- --:--:--  4480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "rdkit is already installed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 ms, sys: 0 ns, total: 2 ms\n",
            "Wall time: 2.07 ms\n",
            "Collecting chainer-chemistry\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/47/99562193229fe60940ef28ef4366d5c16308f76db953350a4ae6f602d32a/chainer-chemistry-0.5.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chainer>=3.0 in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (5.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (3.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (0.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from chainer-chemistry) (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=3.0->chainer-chemistry) (1.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer>=3.0->chainer-chemistry) (3.0.12)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=3.0->chainer-chemistry) (3.7.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=3.0->chainer-chemistry) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->chainer-chemistry) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->chainer-chemistry) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->chainer-chemistry) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->chainer-chemistry) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->chainer-chemistry) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->chainer>=3.0->chainer-chemistry) (41.2.0)\n",
            "Building wheels for collected packages: chainer-chemistry\n",
            "  Building wheel for chainer-chemistry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chainer-chemistry: filename=chainer_chemistry-0.5.0-cp36-none-any.whl size=132520 sha256=616728127f4bf244a7e73b3699dfc3622a207146c85530bee1133a68e4f57f31\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/32/e0/15d059a9218ee5d6306e124aa82b41a63c5fc61885fca277ae\n",
            "Successfully built chainer-chemistry\n",
            "Installing collected packages: chainer-chemistry\n",
            "Successfully installed chainer-chemistry-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMjlFYzToLqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import chainer\n",
        "import numpy\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "from chainer.datasets import split_dataset_random\n",
        "from chainer.datasets import SubDataset\n",
        "from chainer import cuda\n",
        "from chainer import functions as F\n",
        "from chainer import optimizers\n",
        "from chainer import training\n",
        "from chainer import Variable\n",
        "from chainer.iterators import SerialIterator\n",
        "from chainer.training import extensions as E\n",
        "from chainer.training import triggers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "from chainer_chemistry.dataset.converters import concat_mols\n",
        "from chainer_chemistry.dataset.parsers import CSVFileParser\n",
        "from chainer_chemistry.dataset.preprocessors import preprocess_method_dict\n",
        "from chainer_chemistry.datasets import NumpyTupleDataset\n",
        "from chainer_chemistry.models import MLP, NFP, GGNN, SchNet, WeaveNet, RSGCN, Regressor  # NOQA\n",
        "from datetime import datetime\n",
        "starting_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "import requests\n",
        "import json\n",
        "post_url = \"https://hooks.slack.com/services/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\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amFnOs3mpCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_slack(name, text):\n",
        "    requests.post(\n",
        "        post_url,\n",
        "        data=json.dumps(\n",
        "            {\"text\": text,\n",
        "             \"username\": name,\n",
        "             \"icon_emoji\": \":python:\"}))\n",
        "\n",
        "def best_model_save_extension(model_path_best, regressor, protocol):\n",
        "    @training.make_extension()\n",
        "    def _save_pickle(trainer):\n",
        "        regressor.save_pickle(model_path_best, protocol=protocol)\n",
        "    return _save_pickle\n",
        "\n",
        "class GraphConvPredictor(chainer.Chain):\n",
        "    def __init__(self, graph_conv, mlp=None):\n",
        "        \"\"\"Initializes the graph convolution predictor.\n",
        "\n",
        "        Args:\n",
        "            graph_conv: The graph convolution network required to obtain\n",
        "                        molecule feature representation.\n",
        "            mlp: Multi layer perceptron; used as the final fully connected\n",
        "                 layer. Set it to `None` if no operation is necessary\n",
        "                 after the `graph_conv` calculation.\n",
        "        \"\"\"\n",
        "\n",
        "        super(GraphConvPredictor, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.graph_conv = graph_conv\n",
        "            if isinstance(mlp, chainer.Link):\n",
        "                self.mlp = mlp\n",
        "        if not isinstance(mlp, chainer.Link):\n",
        "            self.mlp = mlp\n",
        "    # グラフ畳み込み層を適用したあと, 多層パーセプトロン (あれば) を適用する仕組み\n",
        "    def __call__(self, atoms, adjs):\n",
        "        h = self.graph_conv(atoms, adjs)\n",
        "        if self.mlp:\n",
        "            h = self.mlp(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class MeanAbsError(object):\n",
        "    def __init__(self, scaler=None, out_dir = \"result\"):\n",
        "        \"\"\"Initializes the (scaled) mean absolute error metric object.\n",
        "\n",
        "        Args:\n",
        "            scaler: Standard label scaler.\n",
        "        \"\"\"\n",
        "        self.scaler = scaler\n",
        "        self.out_dir = out_dir\n",
        "\n",
        "    def __call__(self, x0, x1):\n",
        "        if isinstance(x0, Variable):\n",
        "            x0 = cuda.to_cpu(x0.data)\n",
        "        if isinstance(x1, Variable):\n",
        "            x1 = cuda.to_cpu(x1.data)\n",
        "        if self.scaler is not None:\n",
        "            scaled_x0 = self.scaler.inverse_transform(cuda.to_cpu(x0))\n",
        "            scaled_x1 = self.scaler.inverse_transform(cuda.to_cpu(x1))\n",
        "            diff = scaled_x0 - scaled_x1\n",
        "            #print(scaled_x0)\n",
        "            numpy.savetxt(self.out_dir + \"/pred.csv\",scaled_x0)\n",
        "            numpy.savetxt(self.out_dir + \"/meas.csv\",scaled_x1)\n",
        "\n",
        "        else:\n",
        "            diff = cuda.to_cpu(x0) - cuda.to_cpu(x1)\n",
        "            numpy.savetxt(self.out_dir + \"/pred.csv\",cuda.to_cpu(x0))\n",
        "            numpy.savetxt(self.out_dir + \"/meas.csv\",cuda.to_cpu(x1))\n",
        "        return numpy.mean(numpy.absolute(diff), axis=0)[0]\n",
        "\n",
        "\n",
        "class RootMeanSqrError(object):\n",
        "    def __init__(self, scaler=None):\n",
        "        \"\"\"Initializes the (scaled) root mean square error metric object.\n",
        "\n",
        "        Args:\n",
        "            scaler: Standard label scaler.\n",
        "        \"\"\"\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def __call__(self, x0, x1):\n",
        "        if isinstance(x0, Variable):\n",
        "            x0 = cuda.to_cpu(x0.data)\n",
        "        if isinstance(x1, Variable):\n",
        "            x1 = cuda.to_cpu(x1.data)\n",
        "        if self.scaler is not None:\n",
        "            scaled_x0 = self.scaler.inverse_transform(cuda.to_cpu(x0))\n",
        "            scaled_x1 = self.scaler.inverse_transform(cuda.to_cpu(x1))\n",
        "            diff = scaled_x0 - scaled_x1\n",
        "        else:\n",
        "            diff = cuda.to_cpu(x0) - cuda.to_cpu(x1)\n",
        "        return numpy.sqrt(numpy.mean(numpy.power(diff, 2), axis=0)[0])\n",
        "\n",
        "\n",
        "class ScaledAbsError(object):\n",
        "    def __init__(self, scaler=None):\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def __call__(self, x0, x1):\n",
        "        if isinstance(x0, Variable):\n",
        "            x0 = cuda.to_cpu(x0.data)\n",
        "        if isinstance(x1, Variable):\n",
        "            x1 = cuda.to_cpu(x1.data)\n",
        "        if self.scaler is not None:\n",
        "            scaled_x0 = self.scaler.inverse_transform(cuda.to_cpu(x0))\n",
        "            scaled_x1 = self.scaler.inverse_transform(cuda.to_cpu(x1))\n",
        "            diff = scaled_x0 - scaled_x1\n",
        "        else:\n",
        "            diff = cuda.to_cpu(x0) - cuda.to_cpu(x1)\n",
        "        return numpy.mean(numpy.absolute(diff), axis=0)[0]\n",
        "\n",
        "\n",
        "def set_up_predictor(method, n_unit, conv_layers, class_num):\n",
        "    \"\"\"Sets up the graph convolution network  predictor.\n",
        "\n",
        "    Args:\n",
        "        method: Method name. Currently, the supported ones are `nfp`, `ggnn`,\n",
        "                `schnet`, `weavenet` and `rsgcn`.\n",
        "        n_unit: Number of hidden units.\n",
        "        conv_layers: Number of convolutional layers for the graph convolution\n",
        "                     network.\n",
        "        class_num: Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        An instance of the selected predictor.\n",
        "    \"\"\"\n",
        "\n",
        "    predictor = None\n",
        "    mlp = MLP(out_dim=class_num, hidden_dim=n_unit)\n",
        "\n",
        "    if method == 'nfp':\n",
        "        print('Training an NFP predictor...')\n",
        "        nfp = NFP(out_dim=n_unit, hidden_dim=n_unit, n_layers=conv_layers)\n",
        "        predictor = GraphConvPredictor(nfp, mlp)\n",
        "    elif method == 'ggnn':\n",
        "        print('Training a GGNN predictor...')\n",
        "        ggnn = GGNN(out_dim=n_unit, hidden_dim=n_unit, n_layers=conv_layers)\n",
        "        predictor = GraphConvPredictor(ggnn, mlp)\n",
        "    elif method == 'schnet':\n",
        "        print('Training an SchNet predictor...')\n",
        "        schnet = SchNet(out_dim=class_num, hidden_dim=n_unit,\n",
        "                        n_layers=conv_layers)\n",
        "        predictor = GraphConvPredictor(schnet, None)\n",
        "    elif method == 'weavenet':\n",
        "        print('Training a WeaveNet predictor...')\n",
        "        n_atom = 30 #adjust with config.py\n",
        "        n_sub_layer = 1\n",
        "        weave_channels = [50] * conv_layers\n",
        "\n",
        "        weavenet = WeaveNet(weave_channels=weave_channels, hidden_dim=n_unit,\n",
        "                            n_sub_layer=n_sub_layer, n_atom=n_atom)\n",
        "        predictor = GraphConvPredictor(weavenet, mlp)\n",
        "    elif method == 'rsgcn':\n",
        "        print('Training an RSGCN predictor...')\n",
        "        rsgcn = RSGCN(out_dim=n_unit, hidden_dim=n_unit, n_layers=conv_layers)\n",
        "        predictor = GraphConvPredictor(rsgcn, mlp)\n",
        "    else:\n",
        "        raise ValueError('[ERROR] Invalid method: {}'.format(method))\n",
        "    return predictor\n",
        "\n",
        "\n",
        "def parse_arguments():\n",
        "    # Lists of supported preprocessing methods/models.\n",
        "    method_list = ['nfp', 'ggnn', 'schnet', 'weavenet', 'rsgcn']\n",
        "    scale_list = ['standardize', 'none']\n",
        "\n",
        "    # Set up the argument parser.\n",
        "    parser = ArgumentParser(description='Regression on own dataset')\n",
        "    parser.add_argument('--datafile', '-d', type=str,\n",
        "                        default='dataset_train.csv',\n",
        "                        help='csv file containing the dataset')\n",
        "    parser.add_argument('--datafile_test', '-dt', type=str,\n",
        "                        default='dataset_test.csv',\n",
        "                        help='csv file containing the dataset')\n",
        "    parser.add_argument('--method', '-m', type=str, choices=method_list,\n",
        "                        help='method name', default='nfp')\n",
        "    \n",
        "    parser.add_argument('--label', '-l', nargs='+',\n",
        "                        default=['value1', 'value2'],\n",
        "                        help='target label for regression')\n",
        "    \n",
        "    parser.add_argument('--label_t', '-lt', nargs='+',\n",
        "                        default=['value1', 'value2'],\n",
        "                        help='target label for regression')\n",
        "\n",
        "    parser.add_argument('--scale', type=str, choices=scale_list,\n",
        "                        help='label scaling method', default='standardize')\n",
        "    parser.add_argument('--conv-layers', '-c', type=int, default=4,\n",
        "                        help='number of convolution layers')\n",
        "    parser.add_argument('--batchsize', '-b', type=int, default=32,\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--gpu', '-g', type=int, default=-1,\n",
        "                        help='id of gpu to use; negative value means running'\n",
        "                        'the code on cpu')\n",
        "    parser.add_argument('--out', '-o', type=str, default='result',\n",
        "                        help='path to save the computed model to')\n",
        "    parser.add_argument('--epoch', '-e', type=int, default=10,\n",
        "                        help='number of epochs')\n",
        "    parser.add_argument('--unit-num', '-u', type=int, default=16,\n",
        "                        help='number of units in one layer of the model')\n",
        "    parser.add_argument('--seed', '-s', type=int, default=777,\n",
        "                        help='random seed value')\n",
        "    parser.add_argument('--train-data-ratio', '-r', type=float, default=0.8,\n",
        "                        help='ratio of training data w.r.t the dataset')\n",
        "    parser.add_argument('--protocol', type=int, default=2,\n",
        "                        help='pickle protocol version')\n",
        "    parser.add_argument('--model-filename', type=str, default='regressor.pkl',\n",
        "                        help='saved model filename')\n",
        "    return parser.parse_args(args=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YukskLcKnV4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "737670c9-4085-4164-ed7d-4facede1f853"
      },
      "source": [
        "def main():\n",
        "    # Parse the arguments.\n",
        "    args = parse_arguments()\n",
        "    print (\"attributes of argument parser\\n\")\n",
        "    print(vars(args))\n",
        "    best_loss_epoch_list = []\n",
        "\n",
        "    if args.label:\n",
        "        labels = args.label\n",
        "        labels_test = args.label_t\n",
        "        class_num = len(labels) if isinstance(labels, list) else 1\n",
        "        #class_num = len(labels_test) if isinstance(labels_test, list1) else 1\n",
        "    else:\n",
        "        raise ValueError('No target label was specified.')\n",
        "\n",
        "    # Dataset preparation. Postprocessing is required for the regression task.\n",
        "    def postprocess_label(label_list):\n",
        "        return numpy.asarray(label_list, dtype=numpy.float32)\n",
        "\n",
        "    # Apply a preprocessor to the dataset.\n",
        "    print('Preprocessing dataset...')\n",
        "    preprocessor = preprocess_method_dict[args.method]()\n",
        "    parser = CSVFileParser(preprocessor, postprocess_label=postprocess_label,\n",
        "                           labels=labels, smiles_col='SMILES')\n",
        "    dataset = parser.parse(args.datafile)['dataset']\n",
        "    dataset_test = parser.parse(args.datafile_test)['dataset']\n",
        "    #for key, value in\n",
        "    #NumpyTupleDataset.save('data.npz',dataset)\n",
        "\n",
        "    # Scale the label values, if necessary.\n",
        "    if args.scale == 'standardize':\n",
        "        scaler = StandardScaler()\n",
        "        labels = scaler.fit_transform(dataset.get_datasets()[-1])\n",
        "        #numpy.savetxt(\"label_scalar.csv\",labels)\n",
        "        labels_test = scaler.fit_transform(dataset_test.get_datasets()[-1])\n",
        "        #la = scaler.inverse_transform(labels)\n",
        "        #numpy.savetxt(\"label.csv\",la)\n",
        "        dataset = NumpyTupleDataset(*(dataset.get_datasets()[:-1] + (labels,)))\n",
        "        dataset_test = NumpyTupleDataset(*(dataset_test.get_datasets()[:-1] + (labels_test,)))\n",
        "        #print(dataset)\n",
        "    else:\n",
        "        scaler = None\n",
        "\n",
        "    # Split the dataset into training and validation.\n",
        "    train = SubDataset(dataset,0,int(len(dataset)))\n",
        "    print(len(train))\n",
        "    val = SubDataset(dataset_test,0,int(len(dataset_test)))\n",
        "    print(len(val))\n",
        "    #train_data_size = int(len(dataset) * args.train_data_ratio)\n",
        "    #train, val = split_dataset_random(dataset, train_data_size, args.seed)\n",
        "    \"\"\"\n",
        "    print(\"train:\"+str(len(train)))\n",
        "    print(\"test:\"+str(len(val)))\n",
        "    print(test)\n",
        "    for i in range(len(val)):\n",
        "        tes=val[i][2]\n",
        "        #print(float(tes))\n",
        "        with open('label_test.csv','a') as ff:\n",
        "            ff.write(str(tes)+'\\n')\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    #numpy.savetxt('train.csv', train)\n",
        "    #NumpyTupleDataset.save('test.npz', test)\n",
        "\n",
        "    # Set up the predictor.\n",
        "    predictor = set_up_predictor(args.method, args.unit_num,\n",
        "                                 args.conv_layers, class_num)\n",
        "\n",
        "    # Set up the iterator.\n",
        "    train_iter = SerialIterator(train, args.batchsize)\n",
        "    val_iter = SerialIterator(val, args.batchsize, repeat=False, shuffle=False)\n",
        "\n",
        "    # Set up the regressor.\n",
        "    metrics_fun = {'mean_abs_error': MeanAbsError(scaler=scaler, out_dir = args.out),\n",
        "                   'root_mean_sqr_error': RootMeanSqrError(scaler=scaler)}\n",
        "    \n",
        "    regressor = Regressor(predictor, lossfun=F.mean_squared_error,\n",
        "                          metrics_fun=metrics_fun, device=args.gpu)\n",
        "\n",
        "    # Set up the optimizer.\n",
        "    optimizer = optimizers.Adam()\n",
        "    optimizer.setup(regressor)\n",
        "\n",
        "    # Set up the updater.\n",
        "    updater = training.StandardUpdater(train_iter,\n",
        "                                       optimizer,\n",
        "                                       device=args.gpu,\n",
        "                                       converter=concat_mols)\n",
        "\n",
        "    # Set up the trainer.\n",
        "    print('Training...')\n",
        "    #trainer を作成. args.epoch で学習を終了し, args.out に関連データを保存する\n",
        "    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)\n",
        "\n",
        "    # Extension を付加\n",
        "    trainer.extend(E.Evaluator(val_iter, regressor, device=args.gpu,converter=concat_mols))\n",
        "\n",
        "    # 最良のモデルを保存するエクステンション\n",
        "    trigger_best_model = triggers.MinValueTrigger('validation/main/root_mean_sqr_error', trigger=(1, 'epoch'))\n",
        "    model_path_best = os.path.join(args.out, \"best\" + args.model_filename)\n",
        "    trainer.extend(best_model_save_extension(model_path_best, regressor, args.protocol), trigger=trigger_best_model)\n",
        "    trainer.extend(E.LogReport(log_name='log_report'))\n",
        "    trainer.extend(E.PrintReport(['epoch', 'main/loss', 'main/mean_abs_error',\n",
        "                                  'main/root_mean_sqr_error','validation/main/loss',\n",
        "                                  'validation/main/mean_abs_error','validation/main/root_mean_sqr_error',\n",
        "                                  'elapsed_time']))\n",
        "    trainer.extend(E.ProgressBar())\n",
        "    trainer.extend(E.dump_graph(root_name=\"main/loss\", out_name=\"cg.dot\"))\n",
        "    trainer.run()\n",
        "\n",
        "    # Save the regressor's parameters.\n",
        "    model_path = os.path.join(args.out, args.model_filename)\n",
        "    print('Saving the trained model to {}...'.format(model_path))\n",
        "    regressor.save_pickle(model_path, protocol=args.protocol)\n",
        "\n",
        "    # Save the standard scaler's parameters.\n",
        "    if scaler is not None:\n",
        "        with open(os.path.join(args.out, 'scaler.pkl'), mode='wb') as f:\n",
        "            pickle.dump(scaler, f, protocol=args.protocol)\n",
        "    \n",
        "    slack_message = \"method: \" + str(args.method) + \"\\n\" + \"targets:\\n\" + str(args.label)\n",
        "    post_slack (\"best_loss_epoch_list\", \"学習が終了しました\\n\" + slack_message)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--datafile DATAFILE]\n",
            "                             [--datafile_test DATAFILE_TEST]\n",
            "                             [--method {nfp,ggnn,schnet,weavenet,rsgcn}]\n",
            "                             [--label LABEL [LABEL ...]]\n",
            "                             [--label_t LABEL_T [LABEL_T ...]]\n",
            "                             [--scale {standardize,none}]\n",
            "                             [--conv-layers CONV_LAYERS]\n",
            "                             [--batchsize BATCHSIZE] [--gpu GPU] [--out OUT]\n",
            "                             [--epoch EPOCH] [--unit-num UNIT_NUM]\n",
            "                             [--seed SEED]\n",
            "                             [--train-data-ratio TRAIN_DATA_RATIO]\n",
            "                             [--protocol PROTOCOL]\n",
            "                             [--model-filename MODEL_FILENAME]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-74c8fc45-2b89-4dde-aa23-19d34f3dc4ba.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}